import numpy as np
from kmeans import kmeans
from scipy.special import psi,gammaln

class theta():
	def __init__(self,nstates):
		self.k = nstates
		self.m = np.zeros(self.k)
		self.beta= np.ones(self.k)
		self.a = np.ones(self.k)
		self.b = np.ones(self.k)

class vbem_normal():
	def __init__(self,x,nstates):
		self.k = nstates
		self.x = x
		self.prior = theta(self.k)
		self.post = theta(self.k)
		
		self.e_mu = np.zeros((2,self.k))
		self.e_lam = np.zeros((2,self.k))
		
		self.r = np.array([np.random.dirichlet(np.ones(self.k)) for _ in range(self.x.size)])
		self.update_stats()
		self.update_r()
		
		self.lowerbound = -np.inf
	
	def update_stats(self):
		### Bishop pg 477
		nk = self.r.sum(0)
		xbar = np.sum(self.r*self.x[:,None],axis=0) / nk
		sk = np.sum(self.r*(self.x[:,None] - xbar[None,:])**2.,axis=0) / nk
		self.stats = np.array((nk,xbar,sk))
	
	def update_r(self):
		p = self.calc_lnp_xi(self.x)
		p -= p.max(1)[:,None]
		p = np.exp(p)
		# p = p_normal(self.x[:,None],self.post.m[None,:],(self.post.b/self.post.a)[None,:])
		
		self.r = p/(p.sum(1)[:,None])
		
		self.update_stats()
	
	def update(self):
		### Bishop pg 478
		# mu
		self.post.beta = self.prior.beta + self.stats[0]
		self.post.m = self.prior.beta*self.prior.m + self.stats[0]*self.stats[1]
		self.post.m /= self.post.beta
		
		# lam
		self.post.a = self.prior.a + (self.stats[0] + 1.)/2.
		self.post.b = self.prior.b + .5*self.stats[0]*self.stats[2]
		self.post.b += 0.5*self.prior.beta*self.stats[0]/(self.prior.beta+self.stats[0])*(self.stats[1] - self.prior.m)**2.
		
		self.update_lnl_mu()
		self.update_lnl_lam()
		
		self.update_r()
		self.calc_lowerbound()
	
	### See Winn, J. (Thesis) - 2.37-2.39 (pg. 41)
	def update_lnl_mu(self):
		y1 = (self.prior.beta*self.prior.m - self.post.beta*self.post.m) * self.post.m
		y2 = (-self.prior.beta/2. + self.post.beta/2.)*(self.post.m**2. + 1./self.post.beta)
		y3 = .5*(np.log(self.post.beta) - self.post.beta*self.post.m**2. - np.log(self.prior.beta) + self.prior.beta*self.prior.m**2.)
		self.lnl_mu = y1+y2+y3
	
	def update_lnl_lam(self):
		e_lam = self.post.a/self.post.b
		e_lnlam = psi(self.post.a) - np.log(self.post.b)
		
		y1 = (-self.prior.b + self.post.b)*e_lam
		y2 = (self.post.a - self.prior.a)*e_lnlam
		y3 = self.post.a*np.log(self.post.b) - gammaln(self.post.a) - self.prior.a*np.log(self.prior.b) + gammaln(self.prior.a)
		self.lnl_lam = y1+y2+y3
	
	def calc_lnp_xi(self,x):
		e_lam = self.post.a/self.post.b
		e_mu = self.post.m
		e_mu2 = self.post.m**2. + 1./self.post.beta
		e_lnlam = psi(self.post.a) - np.log(self.post.b)
		
		y1 = (e_lam*e_mu)[None,:]*x[:,None]
		y2 = -.5*e_lam[None,:]*x[:,None]**2.
		y3 = .5*(e_lnlam - e_lam*e_mu2 - np.log(2.*np.pi))
		lnp_xi = (y1 + y2 + y3[None,:])
		return lnp_xi
	
	def kl_gauss(self):
		return 0.5 * self.prior.beta * (self.post.m - self.prior.m)**2. + .5 *(self.prior.beta/self.post.beta - 1. - np.log(self.prior.beta/self.post.beta))
	
	def kl_gamma(self):
		return (self.post.a - self.prior.a)*psi(self.post.a) - gammaln(self.post.a) + gammaln(self.prior.a) + self.prior.a*(np.log(self.post.b) - np.log(self.prior.b)) + self.post.a*(self.prior.b - self.post.b)/self.post.b
	
	def calc_lowerbound(self):
		lnp = self.calc_lnp_xi(self.x)
		self.lowerbound = - self.kl_gauss().sum()# - self.lnl_lam.sum()# ((lnp +self.lnl_lam[None,:] + self.lnl_mu[None,:])*self.r).sum()
		# p = self.calc_lnp_xi(self.x).sum() + self.r.sum()
		# self.lowerbound = self.lnl_lam.sum() + self.lnl_mu.sum()
		# self.lowerbound = (self.calc_lnp_xi(self.x).sum(0)).sum() - self.lnl_lam - self.lnl_mu).sum()


def run_vbem(x,nstates,init_kmeans=False,maxiters=1000,threshold=1e-16):
	a = vbem_normal(x,nstates)

	if init_kmeans:
		km = kmeans(x,nstates,nrestarts=10)
		xsort = np.argsort(km.mu[:,0])
		mu = km.mu[:,0][xsort]
		var = km.var[:,0,0][xsort] + 1e-300
		
	else:
		mu = np.random.rand(nstates)*(x.max()-x.min()) + x.min()
		mu.sort() # keep states ordered by mu
		# distinv = 1./np.sqrt((x[:,None] - mu[None,:])**2.)
		# r = distinv/distinv.sum(1)[:,None]
		# r = np.sqrt((x[:,None] - mu[None,:])**2.)
		# xr = r.argmin(1)
		# r *= 0.
		# r[xr] = 1.
		#
		# var = np.sum(r*(x[:,None]-mu[None,:])**2.,axis=0)/np.sum(r,axis=0) +1e-300
		var = np.var(x)
	
	a.prior.m = mu
	a.prior.beta = np.ones(nstates)/var
	a.prior.a = np.ones(nstates)
	a.prior.b = np.ones(nstates)

	lls = np.array((a.lowerbound,))

	for iterations in range(maxiters):
		a.update()
		lls = np.append(lls,a.lowerbound)
		print "%d %.20f %.20f"%(iterations,lls[-1],np.abs(lls[-1] - lls[-2])/np.abs(lls[-1]))
		
		if iterations > 2 and np.abs(lls[-1] - lls[-2])/np.abs(lls[-1]) < threshold:
			
			break
		# if (1+iterations) % 5 == 0:
			# if np.abs((lls[-5:-1] - lls[-1])/lls[-1]).mean() < threshold:
				# break
	return a


#### Testing
import matplotlib.pyplot as plt
from normal_dist import p_normal

n = np.array([200,400])
mu = np.array((5000.,7500.))
x = np.concatenate([np.random.poisson(lam=mu[i],size=n[i]) for i in range(n.size)])

nstates=2
o = kmeans(x,nstates)


# a = vmp_normal(x,nstates)
# a.prior.m = o.mu[:,0]
# a.prior.beta *= 0.01
# a.prior.a = np.ones(nstates)
# a.prior.b = o.var[:,0,0]/100.
#
# for i in range(100):
# 	a.update()
# print a.post.__dict__


hy,hx = plt.hist(x,bins=80,alpha=.5,normed=True)[:2]
xx = np.linspace(0,10000,10000)
# [plt.plot(xx,p_normal(xx,o.mu[i],o.var[i,0,0])*o.pi[i],'k') for i in range(nstates)]
a = [run_vbem(x,2,init_kmeans=True)]
for i in range(4):
	a.append(run_vbem(x,2,init_kmeans=False))
	
cm = plt.cm.spectral
for i in range(len(a)):
	aa = a[i]
	aa.pi = aa.r.sum(0)/aa.r.sum()
	[plt.plot(xx,p_normal(xx,aa.post.m[j],aa.post.b[j]/aa.post.a[j])*aa.pi[j],color=cm(float(i)/float(len(a)-1.))) for j in range(nstates)]
# plt.xlim(hx.min(),hx.max())
plt.show()

print [aa.lowerbound for aa in a]
print [aa.post.m for aa in a]

